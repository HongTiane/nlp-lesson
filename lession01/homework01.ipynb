{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础理论部分\n",
    "\n",
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "（1）语音助手（2）自动驾驶（3）机器人客服\n",
    "\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "Github是一个开源社区，里面有很多优秀的开源项目，其中包含很多AI相关的项目，所以可以在Github查找、学习和使用这些项目。\n",
    "Jupyter and Pycharm是python IDE工具，使用这些工具有助于快速的开发。Jupyter基于IPython提供便捷的交互式开发环境，各种快捷方式，魔术命令。Pycharm主要用于开发大型的python项目。\n",
    "\n",
    "2. What's the Probability Model?\n",
    "以概率论为基础，使用数学方法描述语言的规律\n",
    "\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "1、垃圾邮件过滤\n",
    "\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "用概率来衡量一个句子是否合理能够把复杂的问题变得简单化，编写的代码具有通用性，不必因为规则的改变而修改代码。\n",
    "基于解析和模式匹配的编程不具有通用性和扩展性，一旦规则发生改变就需要修改代码\n",
    "\n",
    "5. What's the Language Model;\n",
    "为自然语言上下文相关的特性建立数学模型\n",
    "\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "1、语音识别\n",
    "2、机器翻译\n",
    "\n",
    "7. What's the 1-gram language model;\n",
    "1-gram language model是一个上下文无关的模型，假设当前出现的概率与前面的词无关\n",
    "\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "1-gram language model的缺点是效果不明显，准确率低\n",
    "1-gram language model的优点是易于理解和计算\n",
    "\n",
    "9. What't the 2-gram model;\n",
    "2-gram model是指句子中的每个词之和前面的一个词有关，即在已知第一个词前提下，第二个词出现的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_sentence(grammar_rules: str, target, stmt_split='=', expr_split='|'):\n",
    "    \"\"\"\n",
    "    产生句子\n",
    "    \"\"\"\n",
    "    grammar_rules_dict = parse_grammar_rules(grammar_rules, stmt_split, expr_split)\n",
    "\n",
    "    return random_genrate_sentence(grammar_rules_dict, target)\n",
    "\n",
    "def parse_grammar_rules(grammar_rules: str, stmt_split='=', expr_split='|'):\n",
    "    \"\"\"\n",
    "    解析语法规则\n",
    "    \"\"\"\n",
    "    grammar_rules_dict = {}\n",
    "    for line in grammar_rules.split('\\n'):\n",
    "        if line:\n",
    "            stmt, expr = line.split(stmt_split)\n",
    "            grammar_rules_dict[stmt.strip()] = expr.split(expr_split)\n",
    "    return grammar_rules_dict;\n",
    "\n",
    "def random_genrate_sentence(grammar_rules: dict, target):\n",
    "    \"\"\"\n",
    "    随机产生句子\n",
    "    \"\"\"\n",
    "    if target in grammar_rules:\n",
    "        candidate = random.choice(grammar_rules[target])\n",
    "        return ''.join(random_genrate_sentence(grammar_rules, e.strip()) for e in candidate.split())\n",
    "    else:\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zhon.hanzi\n",
    "def filter_invalid_char(string: str):\n",
    "    chinese_punctuation = zhon.hanzi.punctuation\n",
    "    english_punctuation = ',.?;:\\'\"`~!'\n",
    "    special_char = r'<>/\\\\|\\[\\]{}@#\\$%\\^&\\*\\(\\)-_\\+=\\n'\n",
    "    return re.sub('(?P<punctuation>[{}]|[{}])|(?P<special_char>[{}])'.format(chinese_punctuation, english_punctuation, special_char), '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很燃战争的场面很宏伟人与人的打and machineany斗很精彩看得人热血沸腾为同胞而战'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_invalid_char('很燃！战争的场,面..很?$%()宏伟，人与人的打and machine,any斗很+++---精彩！看得人热血沸腾，为同胞而战！\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_invalid_char('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def clean_data_set(movie_comments_file_path, train_file_path, output_file_path):\n",
    "    with open(movie_comments_file_path, encoding='utf-8') as movie_comments_file, open(train_file_path, encoding='utf-8') as train_file, open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        movie_comments_reader = csv.DictReader(movie_comments_file)\n",
    "        for row in movie_comments_reader:\n",
    "            output_file.write(filter_invalid_char(row['comment']))\n",
    "            \n",
    "        for line in train_file:\n",
    "            output_file.write(filter_invalid_char(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def generate_2_gram_words_count(sample_file_path: str, unit_length=1000000):\n",
    "    words_count = {}\n",
    "    two_gram_words_count = {}\n",
    "    with open(sample_file_path, encoding='utf-8') as sample_file:\n",
    "        sample = sample_file.read()\n",
    "        begin_index = 0\n",
    "        while begin_index < len(sample):\n",
    "            words = cut(sample[begin_index:(begin_index + unit_length)])\n",
    "            _generate_2_gram_words_count(words, words_count, two_gram_words_count)\n",
    "            begin_index = begin_index + unit_length\n",
    "    return words_count, two_gram_words_count\n",
    "\n",
    "def cut(sentence):\n",
    "    return list(jieba.cut(sentence))\n",
    "\n",
    "def _generate_2_gram_words_count(words: list, words_count: dict, two_gram_words_count: dict):\n",
    "    _words_count = Counter(words)\n",
    "    for w, f in _words_count.most_common():\n",
    "        if w in words_count:\n",
    "            words_count[w] = words_count[w] + f\n",
    "        else:\n",
    "            words_count[w] = f\n",
    "    \n",
    "    tow_gram_words = [words[i] + words[i + 1] for i in range(0, len(words) - 1)]\n",
    "    _tow_gram_words_count = Counter(tow_gram_words)\n",
    "    for tgw, f in _tow_gram_words_count.most_common():\n",
    "        if tgw in two_gram_words_count:\n",
    "            two_gram_words_count[tgw] = two_gram_words_count[tgw] + f\n",
    "        else:\n",
    "            two_gram_words_count[tgw] = f\n",
    "            \n",
    "def calculate_probability_of_sentence_by_two_gram(sentence: str, words_count: dict, two_gram_words_count: dict):\n",
    "    \"\"\"\n",
    "    Add-one (Laplace) Smoothing\n",
    "    \"\"\"\n",
    "    words = cut(sentence)\n",
    "    probability = 1\n",
    "    v = len(words_count)\n",
    "    probability *= (words_count[words[0]] if words[0] in words_count else 0  + 1) / (sum(words_count.values()) + v)\n",
    "    for i in range(0, len(words) - 1):\n",
    "        w1w2 = words[i] + words[i+1]\n",
    "        probability *= (two_gram_words_count[w1w2] if w1w2 in two_gram_words_count else 0 + 1) / (words_count[words[i]] if words[i] in words_count else 0 + v)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_comments_file_path = 'D:\\\\Users\\\\ZengYubin\\\\study\\\\nlp-coures\\\\lesson01\\\\assignment\\\\data\\\\movie_comments.csv'\n",
    "train_file_path = 'D:\\\\Users\\\\ZengYubin\\\\study\\\\nlp-coures\\\\lesson01\\\\assignment\\\\data\\\\train.txt'\n",
    "output_file_path = 'D:\\\\Users\\\\ZengYubin\\\\study\\\\nlp-coures\\\\lesson01\\\\assignment\\\\data\\\\output.txt'\n",
    "clean_data_set(movie_comments_file_path, train_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZENGYU~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.121 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "words_count, two_gram_words_count = generate_2_gram_words_count(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163552\n",
      "['的', '了', ' ', '是', '我', '都', '电影', '看', '也', '和', '在', '很', '不', '有', '就', '人', '好', '啊', '你', '还', '这', '一个', '还是', '但', '没有', '让', '就是', '剧情', '故事', '说', '喜欢', '太', '又', '他', '给', '没', '吧', '到', '上', '得', '一部', '能', '对', '被', '最后', '最', '这个', '多', '真的', '什么', '不错', '要', '不是', '可以', '觉得', '好看', '自己', '拍', '这部', '想', '感觉', '去', '把', '与', '演技', '导演', '里', '中', '会', '这么', '着', '但是', '片子', '那', '更', '大', '那么', '挺', '有点', '再', '这种', '小', '比', '这样', '真是', '而', '个', '时候', '不过', '演员', '吗', '我们', '虽然', '爱', '看到', '来', '知道', '片', '却', '戏']\n",
      "1766417\n",
      "['  ', '的电影', '都是', '让人', '看的', '也是', '的人', '看了', '的是', '的时候', '的故事', '看完', '我的', '了 ', '让我', '这部电影', '的感觉', '了我', '好的', '这样的', '不知道', '很好', '电影的', '人的', '这是', '我觉得', '自己的', '是我', '的片子', '的一部', '拍的', '的剧情', '★★', '喜欢的', '的 ', '中的', '他的', '是个', '的演技', '好看的', '了吧', '都有', '最好的', '还不错', '我就', '我也', '的我', '都很', '去看', '很有', '我是', '真的是', '不错的', '的青春', '你的', '也很', '的好', ' 我', '不喜欢', '人都', '里的', '最后的', '的爱情', '也不', '一个人', '上的', '的都', '多了', '才是', '看得', '很喜欢', '都在', '都不', '的地方', '的很', '的角色', '还能', '了一个', '死了', '的作品', '很不错', '还可以', '我都', '太多', '也就', '的东西', '就像', '在一起', '到了', '动作戏', '啊 ', '超级英雄', '一部电影', '是一个', '是一部', '笑点', '最大的', '的表演', '一样的', '实在是']\n"
     ]
    }
   ],
   "source": [
    "print(len(words_count))\n",
    "print(list(words_count.keys())[:100])\n",
    "print(len(two_gram_words_count))\n",
    "print(list(two_gram_words_count.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_one = \"\"\"\n",
    "角色活动 = 角色 动作1 地点 动作2 角色 内容\n",
    "角色 = 老师 | 学生 | 医生 | 病人\n",
    "动作1 = 在 | 来 | 回 | 去\n",
    "地点 = 学校 | 教室 | 医院 | 病房\n",
    "动作2 = 教 | 听 | 看 | 为 | 请教 | 询问 | 帮\n",
    "内容 = 学习 | 上课 | 看病 | 做作业 | 病情 | 解决问题 | 读书\n",
    "\"\"\"\n",
    "grammar_tow = \"\"\"\n",
    "智能客服 = 打招呼 介绍 询问\n",
    "打招呼 = 您好 | 你好 | 早上好 | 下午好 | 晚上好 | 晚安 | 早安\n",
    "介绍 = 我是 名称 | 名称 为你服务\n",
    "名称 = 小美 | 小冰 | 小娜 | 小爱\n",
    "询问 = 称呼 有什么需要 事情 | 称呼 有什么 事情 | 我能为 称呼 做什么 | 称呼 需要做什么 | 请赐教\n",
    "称呼 = 你 | 您\n",
    "事情 = 帮忙的 | 查询的 | 困难 | 困惑 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'学生去病房询问老师做作业'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(grammar_one, '角色活动')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'晚安我是小娜我能为您做什么'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(grammar_tow, '智能客服')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_sentences(n):\n",
    "    grammars = [{'grammar_rules': grammar_one, 'target': '角色活动'}, {'grammar_rules': grammar_tow, 'target': '智能客服'}]\n",
    "    sentences = []\n",
    "    for _ in range(0, n):\n",
    "        grammar = random.choice(grammars)\n",
    "        sentences.append(generate_sentence(grammar['grammar_rules'], grammar['target']))\n",
    "    return sentences\n",
    "\n",
    "def calculate_probability_of_sentence(sentence: str):\n",
    "    return calculate_probability_of_sentence_by_two_gram(sentence, words_count, two_gram_words_count)\n",
    "\n",
    "def generate_best_sentence(n=10):\n",
    "    probability_of_sentences = [(sentence, calculate_probability_of_sentence(sentence)) for sentence in generate_n_sentences(n)]\n",
    "    print(probability_of_sentences)\n",
    "    return sorted(probability_of_sentences, key=lambda p: p[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['病人来教室教学生读书',\n",
       " '学生回病房听学生病情',\n",
       " '晚上好小美为你服务您有什么需要查询的',\n",
       " '晚安小爱为你服务我能为您做什么',\n",
       " '晚安我是小冰请赐教',\n",
       " '早安小冰为你服务我能为你做什么',\n",
       " '学生来学校教病人读书',\n",
       " '学生在病房为老师读书',\n",
       " '早安小冰为你服务你需要做什么',\n",
       " '老师在教室看学生学习']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n_sentences(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "病人来学校询问学生看病\n",
      "5.543775988564482e-17\n"
     ]
    }
   ],
   "source": [
    "sentence = generate_sentence(grammar_one, '角色活动')\n",
    "print(sentence)\n",
    "print(calculate_probability_of_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('老师来医院听病人病情', 1.5270346492099765e-17), ('早安小美为你服务请赐教', 6.489086577600779e-22), ('您好我是小娜您有什么需要困难', 1.9259533807049488e-25), ('早上好我是小冰您有什么困难', 7.028766862882715e-22), ('晚安我是小冰你需要做什么', 1.5405350203473182e-20), ('晚上好我是小美我能为你做什么', 1.4254361285541643e-27), ('晚上好小美为你服务你有什么查询的', 3.570810891794637e-27), ('晚安小冰为你服务请赐教', 6.489086577600779e-22), ('病人去医院听病人学习', 2.476591768007258e-17), ('老师去学校听医生学习', 7.344807402206046e-19)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('病人去医院听病人学习', 2.476591768007258e-17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这个模型有什么问题？ 你准备如何提升？\n",
    "这个模型的效果不够好，比如“早上好我是小冰您有什么困难”比“学生去医院为病人病情”更接近人类的语言。\n",
    "出现这个问题的原因可能是数据集不够大或者需要改进语言模型：\n",
    "数据集不够大，可以增加数据集进行训练\n",
    "改进语言模型，可以使用3-Gram模型、修改平滑算法等（在这个模型中使用的是Add-one (Laplace) Smoothing）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
